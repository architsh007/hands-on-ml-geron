# Chapter 5 ‚Äî Support Vector Machines (SVM)

This chapter explores Support Vector Machines from a geometric and optimization perspective.
The goal is to understand not only how to use SVMs, but why they generalize well and how
their hyperparameters control model complexity.

---

## üìå Learning Objectives

- Understand margin maximization
- Differentiate hard-margin and soft-margin SVM
- Interpret the role of C (regularization)
- Understand support vectors and constraint optimization
- Apply the kernel trick for nonlinear classification
- Analyze RBF and polynomial kernels
- Implement Support Vector Regression (SVR)
- Understand SVM scalability limitations

---

## üìì Notebooks Overview

### 1Ô∏è‚É£ Linear SVM: Hard vs Soft Margin
- Maximum margin classifier
- Slack variables
- Effect of C on bias‚Äìvariance tradeoff
- Visualization of decision boundaries

### 2Ô∏è‚É£ Kernel Trick & Nonlinear SVM
- Why linear SVM fails on nonlinear data
- Polynomial kernel
- RBF kernel
- Hyperparameter interaction (C & gamma)
- Visual analysis of overfitting and underfitting

### 3Ô∏è‚É£ Support Vector Regression (SVR)
- Œµ-insensitive loss
- Comparison with MSE
- Interpretation of support vectors in regression

### 4Ô∏è‚É£ SVM vs Logistic Regression
- Compares two margin-based linear classifiers from theoretical and practical perspectives.
- Logistic loss vs hinge loss
- Margin maximization vs probabilistic modeling
- Robustness to noise
- Scalability comparison
- When to prefer SVM vs Logistic Regression

---

## üß† Key Insights

- Only support vectors determine the decision boundary
- C controls the tradeoff between margin width and constraint violations
- Gamma controls locality of influence in the RBF kernel
- Kernel trick enables implicit high-dimensional feature mappings
- SVM complexity depends on the number of support vectors
- Logistic regression produces calibrated probabilities, SVM does not
- Kernel SVMs scale poorly for very large datasets
- Linear SVM is highly effective in high-dimensional sparse spaces (e.g., text)

---

## ‚öôÔ∏è Engineering Tradeoffs

| Aspect                 | Logistic Regression        | SVM                       |
| ---------------------- | -------------------------- | ------------------------- |
| Loss Function          | Logistic loss              | Hinge loss                |
| Output                 | Probabilities              | Margins                   |
| Robustness to Outliers | Moderate                   | High (margin-based)       |
| Scalability            | Excellent                  | Moderate to Poor (kernel) |
| Nonlinear Extension    | Manual feature engineering | Kernel trick              |

---

## üõ† Tools Used

- Python
- NumPy
- Matplotlib
- Scikit-learn