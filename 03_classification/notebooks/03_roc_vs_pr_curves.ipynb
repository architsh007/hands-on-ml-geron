{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1008dd19",
   "metadata": {},
   "source": [
    "# ROC Curves vs Precision–Recall Curves\n",
    "\n",
    "This notebook compares ROC curves and Precision–Recall (PR) curves for evaluating\n",
    "binary classifiers.\n",
    "\n",
    "Using MNIST digit classification (5 vs not-5), we show:\n",
    "- how ROC and PR curves are constructed\n",
    "- what each curve measures\n",
    "- why ROC curves can be misleading under class imbalance\n",
    "- when PR curves provide more actionable insight\n",
    "\n",
    "The goal is to understand *what question each curve answers*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2155cf",
   "metadata": {},
   "source": [
    "# ROC Curves vs Precision–Recall Curves\n",
    "\n",
    "This notebook compares ROC curves and Precision–Recall (PR) curves for evaluating\n",
    "binary classifiers.\n",
    "\n",
    "Using MNIST digit classification (5 vs not-5), we show:\n",
    "- how ROC and PR curves are constructed\n",
    "- what each curve measures\n",
    "- why ROC curves can be misleading under class imbalance\n",
    "- when PR curves provide more actionable insight\n",
    "\n",
    "The goal is to understand *what question each curve answers*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6620a",
   "metadata": {},
   "source": [
    "## Problem Context\n",
    "\n",
    "We consider an imbalanced binary classification problem:\n",
    "- Positive class: digit \"5\"\n",
    "- Negative class: all other digits (~90%)\n",
    "\n",
    "In such settings, evaluation metrics must be chosen carefully, as some metrics\n",
    "can hide critical failure modes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_recall_curve\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b744b",
   "metadata": {},
   "source": [
    "## Training a Linear Classifier\n",
    "\n",
    "We use a simple linear classifier to focus on evaluation rather than\n",
    "model complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd0436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_scores = sgd_clf.decision_function(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722dca0c",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    "\n",
    "The ROC curve plots:\n",
    "- True Positive Rate (Recall)\n",
    "- against False Positive Rate\n",
    "\n",
    "for all possible decision thresholds.\n",
    "\n",
    "ROC curves measure *ranking quality*, not decision quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5069332",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, roc_thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Classifier\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d5f92",
   "metadata": {},
   "source": [
    "## Area Under the ROC Curve (AUC)\n",
    "\n",
    "AUC measures the probability that a randomly chosen positive instance\n",
    "is ranked higher than a randomly chosen negative instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train, y_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160bdc20",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "A high AUC indicates good separation between classes.\n",
    "However, ROC curves do not reflect the absolute number of false positives,\n",
    "which can be problematic in highly imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb3889d",
   "metadata": {},
   "source": [
    "## Precision–Recall Curve\n",
    "\n",
    "Precision–Recall curves focus explicitly on the positive class and are\n",
    "more sensitive to class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6252bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, pr_thresholds = precision_recall_curve(y_train, y_scores)\n",
    "\n",
    "plt.plot(recalls, precisions, linewidth=2)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c555b1",
   "metadata": {},
   "source": [
    "## ROC vs PR: Key Differences\n",
    "\n",
    "- ROC curves measure how well the model ranks positives above negatives.\n",
    "- PR curves measure how reliable positive predictions are.\n",
    "\n",
    "In imbalanced problems:\n",
    "- ROC curves can look deceptively good\n",
    "- PR curves reveal the true cost of false positives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd695b",
   "metadata": {},
   "source": [
    "### Why ROC Can Be Misleading Under Class Imbalance\n",
    "\n",
    "When negative samples dominate, the false positive rate can remain low\n",
    "even when the absolute number of false positives is large.\n",
    "\n",
    "As a result, ROC curves may suggest strong performance while the model\n",
    "produces many incorrect positive predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890aa20f",
   "metadata": {},
   "source": [
    "## When to Use ROC vs PR Curves\n",
    "\n",
    "- Use ROC curves to compare ranking quality between classifiers.\n",
    "- Use PR curves when the positive class is rare and false positives are costly.\n",
    "\n",
    "Metric choice should always reflect the problem context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38659722",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- ROC curves evaluate ranking ability across thresholds.\n",
    "- AUC has a clear probabilistic interpretation but ignores decision costs.\n",
    "- Precision–Recall curves focus on positive-class performance.\n",
    "- For imbalanced problems, PR curves often provide more actionable insight.\n",
    "- Evaluation metrics are not neutral; they encode assumptions and priorities.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
